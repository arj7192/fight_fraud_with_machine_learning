{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e8a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the notebook width to 100% of the screen\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e356fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763f5e96",
   "metadata": {},
   "source": [
    "### data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b955c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Chapter03/phishing_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c56f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4901736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    X = df[[c for c in df.columns if c not in [\"id\", \"CLASS_LABEL\"]]]\n",
    "    y = df[\"CLASS_LABEL\"]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7468f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data(\"../Chapter03/phishing_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ce77c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf10a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1be7476",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c152c8bd",
   "metadata": {},
   "source": [
    "### data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76b2378",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ee104",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isnull().any(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.isnull().any(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcaf192",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y[~X.duplicated()].reset_index(drop=True) \n",
    "X = X[~X.duplicated()].reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d6f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf04a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=1e-10\n",
    "X = (X-X.min())/(X.max()-X.min()+eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1538b1d8",
   "metadata": {},
   "source": [
    "### model selection training evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81190c35",
   "metadata": {},
   "source": [
    "#### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3738a67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_data(\"../Chapter03/phishing_dataset.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# initialize a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit and transform the training set\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# transform the testing set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression (C=1.0, max_iter=1000)\n",
    "\n",
    "# Train the model on the fraud dataset\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict the outcomes for the testing set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(\"Total time:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f075cca",
   "metadata": {},
   "source": [
    "#### random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6a4d67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Define the parameter space for the model\n",
    "param_space = {'C': uniform(0, 10), 'max_iter': randint(500, 2000)}\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define the random search object\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_space, n_iter=10, cv=5, random_state=42)\n",
    "\n",
    "# Fit the random search object on the scaled training set\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:', random_search.best_params_)\n",
    "\n",
    "# Predict the outcomes for the testing set using the best model\n",
    "y_pred = random_search.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# return the trained model\n",
    "model = random_search.best_estimator_\n",
    "\n",
    "print(\"Total time:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9a61d",
   "metadata": {},
   "source": [
    "#### knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4efa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Set the parameters to search over\n",
    "param_dist = {'n_neighbors': randint(1, 30),\n",
    "              'weights': ['uniform', 'distance']}\n",
    "\n",
    "# Create a KNN model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Run randomized search\n",
    "rand_search = RandomizedSearchCV(knn, param_distributions=param_dist, n_iter=10, cv=5, random_state=42)\n",
    "\n",
    "# Fit the randomized search to the training data\n",
    "rand_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:', rand_search.best_params_)\n",
    "\n",
    "# Predict the outcomes for the testing set\n",
    "y_pred = rand_search.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# return the trained model\n",
    "model = rand_search.best_estimator_\n",
    "\n",
    "print(\"Total time:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996cbabf",
   "metadata": {},
   "source": [
    "#### decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51707ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Set the parameters to search over\n",
    "param_dist = {'max_depth': randint(1, 30)}\n",
    "\n",
    "# Create a Decision Tree model\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Run randomized search\n",
    "rand_search = RandomizedSearchCV(dt, param_distributions=param_dist, n_iter=10, cv=5, random_state=42)\n",
    "\n",
    "# Fit the randomized search to the training data\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:', rand_search.best_params_)\n",
    "\n",
    "# Predict the outcomes for the testing set\n",
    "y_pred = rand_search.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# return the trained model\n",
    "model = rand_search.best_estimator_\n",
    "\n",
    "print(\"Total time:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5993b31",
   "metadata": {},
   "source": [
    "#### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d7aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set the parameters to search over\n",
    "param_dist = {'max_depth': randint(1, 30), 'n_estimators': randint(10, 100)}\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Run randomized search\n",
    "rand_search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=10, cv=5, random_state=42)\n",
    "\n",
    "# Fit the randomized search to the training data\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:', rand_search.best_params_)\n",
    "\n",
    "# Predict the outcomes for the testing set\n",
    "y_pred = rand_search.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# return the trained model\n",
    "model = rand_search.best_estimator_\n",
    "\n",
    "print(\"Total time:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c53ba3f",
   "metadata": {},
   "source": [
    "#### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864bde66",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Set the parameters to search over\n",
    "param_dist = {'max_depth': randint(1, 30),\n",
    "              'learning_rate': [0.01, 0.05, 0.1, 0.25, 0.5],\n",
    "              'n_estimators': randint(10, 100)}\n",
    "\n",
    "# Create an XGBoost model\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "# Run randomized search\n",
    "rand_search = RandomizedSearchCV(xgb, param_distributions=param_dist, n_iter=10, cv=5, random_state=42)\n",
    "\n",
    "# Fit the randomized search to the training data\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:', rand_search.best_params_)\n",
    "\n",
    "# Predict the outcomes for the testing set\n",
    "y_pred = rand_search.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# return the trained model\n",
    "model = rand_search.best_estimator_\n",
    "\n",
    "# return the trained model\n",
    "model = rand_search.best_estimator_\n",
    "\n",
    "print(\"Total time:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804af3ce",
   "metadata": {},
   "source": [
    "#### kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad83ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# initialize a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define the parameter space for the model\n",
    "param_space = {'max_iter': np.arange(50, 2001, 50)}\n",
    "\n",
    "# Create a KMeans model\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "\n",
    "# Define the random search object\n",
    "random_search = RandomizedSearchCV(kmeans, param_distributions=param_space, n_iter=10, cv=5, random_state=42)\n",
    "\n",
    "# Train the model on the data\n",
    "random_search.fit(X_scaled)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:', random_search.best_params_)\n",
    "\n",
    "# Get the predicted labels for the data\n",
    "y_pred = random_search.predict(X_scaled)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = max(accuracy_score(y, y_pred), accuracy_score(y, (1-y_pred)))\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# return the trained model\n",
    "model = random_search.best_estimator_\n",
    "\n",
    "print(\"Total time:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c562c5",
   "metadata": {},
   "source": [
    "#### pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b6974",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# define the pipeline\n",
    "pipe = Pipeline([\n",
    "    ('pca', PCA(n_components=5)),\n",
    "    ('xgb', XGBClassifier())\n",
    "])\n",
    "\n",
    "# Set the parameters to search over\n",
    "param_dist = {'xgb__max_depth': randint(1, 30),\n",
    "              'xgb__learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "              'xgb__n_estimators': randint(10, 100)}\n",
    "\n",
    "# Run randomized search with cross-validation\n",
    "rand_search = RandomizedSearchCV(pipe, param_distributions=param_dist, n_iter=10, cv=5, random_state=42)\n",
    "\n",
    "# Fit the randomized search to the training data\n",
    "rand_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:', rand_search.best_params_)\n",
    "\n",
    "# Predict the outcomes for the testing set\n",
    "y_pred = rand_search.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# return the trained model\n",
    "best_pca, best_xgb = rand_search.best_estimator_\n",
    "\n",
    "print(\"Total time:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d016be7",
   "metadata": {},
   "source": [
    "#### model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_simple(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    X = df[[\"NumDash\"]]\n",
    "    y = df[\"CLASS_LABEL\"]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca8aff4",
   "metadata": {},
   "source": [
    "##### train simpler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e88d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "X, y = load_data_simple(\"../Chapter03/phishing_dataset.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set the hyperparameters\n",
    "params = {'max_depth': 7,\n",
    "          'learning_rate': 0.5,\n",
    "          'n_estimators': 92}\n",
    "\n",
    "# Create an XGBoost model\n",
    "xgb = XGBClassifier(**params)\n",
    "\n",
    "# Train the model on the phishing dataset\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the outcomes for the testing set\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(\"Total time:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43358ec4",
   "metadata": {},
   "source": [
    "##### save and load model using joblib and make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683cdfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(xgb, \"xgb_model.joblib\")\n",
    "\n",
    "# Load the saved model\n",
    "xgb = joblib.load(\"xgb_model.joblib\")\n",
    "\n",
    "numdash_data = [[1]]\n",
    "X_numdash = np.array(numdash_data)\n",
    "\n",
    "start_time = time.time()\n",
    "# Make predictions on the NumDash data\n",
    "predictions = xgb.predict(X_numdash)\n",
    "print(\"Total time:\", time.time()-start_time)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc3379",
   "metadata": {},
   "source": [
    "##### save and load model using pickle and make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251fc598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model as a pickle file\n",
    "with open('xgb_model.pkl', 'wb') as f:\n",
    "    pickle.dump(xgb, f)\n",
    "\n",
    "# Load the saved model\n",
    "with open('xgb_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Load NumDash data and prepare it for inference\n",
    "numdash_data = [[10]]\n",
    "X_numdash = np.array(numdash_data)\n",
    "\n",
    "start_time = time.time()\n",
    "# Make predictions using the loaded model\n",
    "y_pred = model.predict(X_numdash)\n",
    "print(\"Total time:\", time.time()-start_time)\n",
    "\n",
    "# Print the predictions\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85740893",
   "metadata": {},
   "source": [
    "##### make prediction using rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77e9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Make predictions using the loaded model\n",
    "y_pred = 1>2\n",
    "print(\"Total time:\", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e5b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
